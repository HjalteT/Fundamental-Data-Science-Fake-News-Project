{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hjalt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hjalt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hjalt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading csv using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1, Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    clean_text() takes a dataframe as input. It lowercases everything\n",
    "    followed by applying reg-ex patterns to the dataframe. \\n\n",
    "    The reg-ex patterns purpose is to match specific patterns in the dataframe that we wish to replace. \\n\n",
    "    All urls, emails, dates, numbers and timestamps are replaced by: <URL>, <EMAIL>, <DATE>, <TIME> and <NUM>\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.apply(lambda x: x.str.lower() if isinstance(x, str) == 'object' else x) #lowercase everything\n",
    "\n",
    "    #reg-ex patterns\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    email_pattern = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+')\n",
    "    numbers_pattern = re.compile(r'[0-9]')\n",
    "    days = r'([0-3]?\\d)'\n",
    "    months = r'(0?[1-9]|11|12|10)'\n",
    "    years = r'((19|20)\\d\\d)'\n",
    "    delimiter = r'([\\/\\-\\._])?' \n",
    "    timestamp = r'([^:][0-2]?\\d):([0-5]?\\d):([0-5]?\\d).([\\d]*)'\n",
    "    dates = f\"\"\"\n",
    "            # YYYY-MM-DD\n",
    "            ({years}{delimiter}{months}{delimiter}{days})\n",
    "            |\n",
    "            # YYYY-DD-MM\n",
    "            ({years}{delimiter}{days}{delimiter}{months})\n",
    "            |\n",
    "            # DD-MM-YYYY\n",
    "            ({days}{delimiter}{months}{delimiter}{years})\n",
    "            |\n",
    "            # MM-DD-YYYY\n",
    "            ({months}{delimiter}{days}{delimiter}{years})\n",
    "            \"\"\"\n",
    "\n",
    "    date_pattern = re.compile(dates, re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
    "\n",
    "    #applying reg-ex patterns\n",
    "    df[column] = df[column].apply(lambda x: re.sub(url_pattern, '<URL>', str(x))) \n",
    "    df[column] = df[column].apply(lambda x: re.sub(email_pattern, '<EMAIL>', str(x))) \n",
    "    df[column] = df[column].apply(lambda x: re.sub(date_pattern, '<DATE>', str(x))) \n",
    "    df[column] = df[column].apply(lambda x: re.sub(timestamp, '<TIME>', str(x))) \n",
    "    df[column] = df[column].apply(lambda x: re.sub(numbers_pattern, '<NUM>', str(x)))\n",
    "\n",
    "    #removing multiple white spaces, tabs, or new lines\n",
    "    df = df.applymap(lambda x: re.sub('\\s+',' ', str(x)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words_in_list(df, column):\n",
    "    unique_words = set() \n",
    "    for string in df[column]:\n",
    "        words = string.split()\n",
    "        unique_words.update(words) \n",
    "    return len(unique_words)\n",
    "\n",
    "def count_unique_words_in_list_guesser(df, column):\n",
    "    unique_words = Counter()\n",
    "    for string in df[column]:\n",
    "        words = string.split()\n",
    "        unique_words.update(words) \n",
    "    return unique_words\n",
    "\n",
    "def count_uniques_after_clean(df, column):\n",
    "    unique_words = Counter()\n",
    "    for words in df[column]:\n",
    "        for word in words:\n",
    "            if isinstance(word, str) and word.isalpha():\n",
    "                unique_words.update([word])\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing, removing stopwords and stemming functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df):\n",
    "    filtered_text = ''.join([char for char in str(df) if char.isalpha() or char.isspace() or char == '<' or char == '>'])\n",
    "    return filtered_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [word for word in sentence if word not in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    return stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df, column, tokeniz = False, stopw = False, stemming = False):\n",
    "    if tokeniz:\n",
    "        df[column] = df[column].apply(tokenize_df)\n",
    "    if stopw:\n",
    "        df[column] = df[column].apply(lambda x: remove_stopwords(x) if isinstance(x, list) else x)\n",
    "    if stemming:\n",
    "        df[column] = df[column].apply(lambda x: stem_(x) if isinstance(x, list) else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_text(df, 'content')\n",
    "words_uncl = count_unique_words_in_list(df_clean, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pre_process(df_clean, \"content\", tokeniz=True, stopw=True)\n",
    "words_clean = len(count_uniques_after_clean(df_clean, \"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pre_process(df_clean, \"content\", stemming=True)  \n",
    "words_stemming = len(count_uniques_after_clean(df_clean, \"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary before any preprocessing were: 28968 unique words\n",
      "Vocabulary when removing stopwords were: 19441 unique words\n",
      "The reduction rate is 32.89\n",
      "Vocabulary when stemming the text were: 17817 unique words\n",
      "Reduction rate is 8.35\n"
     ]
    }
   ],
   "source": [
    "reduction_stopwords = (1 - words_clean/words_uncl)*100\n",
    "reduction_stemming  = (1 - words_stemming/words_clean)*100\n",
    "print(\"Vocabulary before any preprocessing were:\", words_uncl, \"unique words\")\n",
    "print(\"Vocabulary when removing stopwords were:\", words_clean, \"unique words\")\n",
    "print(\"The reduction rate is\", round(reduction_stopwords,2))\n",
    "print(\"Vocabulary when stemming the text were:\", words_stemming, \"unique words\")\n",
    "print(\"Reduction rate is\", round(reduction_stemming,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1, Task 2 and Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_labels_995(dataframe):\n",
    "    replace_fake = re.compile(r'(fake)|(junksci)')\n",
    "    replace_reliable = re.compile(r'(reliable)|(political)')\n",
    "    replace_omitted = re.compile(r'(hate)|(conspiracy)|(satire)|(rumor)|(2018-02-10 13:43:39.521661)|(unreliable)|(nan)|(unknown)|(bias)|(clickbait)|(type)')\n",
    "    dataframe['type'] = dataframe['type'].apply(lambda x: re.sub(replace_fake, 'fake', str(x)))\n",
    "    dataframe['type'] = dataframe['type'].apply(lambda x: re.sub(replace_reliable, 'reliable', str(x)))\n",
    "    dataframe['type'] = dataframe['type'].apply(lambda x: re.sub(replace_omitted, 'omitted', str(x)))\n",
    "    remove_omitted = dataframe[(dataframe['type'] == 'omitted')].index\n",
    "    dataframe.drop(remove_omitted, inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "def process_data_in_chunks(file_path, chunk_size=10000):\n",
    "    # Accumulate chunks into a list\n",
    "    # Read the data in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Process the chunk as needed\n",
    "        processed_chunk = clean_text(chunk, 'content')\n",
    "        processed_chunk = pre_process(processed_chunk, \"content\", tokeniz= True, stopw= True, stemming=True)\n",
    "        chunks = []\n",
    "        chunks.append(processed_chunk)\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        df.to_csv('995_cleaned_preprocessed.csv', mode='a', index= False)\n",
    "        del processed_chunk\n",
    "        del chunk\n",
    "        del chunks\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data_in_chunks(\"995,000_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before grouping labels:\n",
      "political  length :  3361.3220781624323\n",
      "fake  length :  2660.054031635251\n",
      "satire  length :  1574.5808510638299\n",
      "reliable  length :  3057.8518191467947\n",
      "conspiracy  length :  2347.6910927513\n",
      "unreliable  length :  2232.731178535039\n",
      "bias  length :  2833.724638225051\n",
      "rumor  length :  1776.0850208167242\n",
      "unknown  length :  4356.339252078835\n",
      "nan  length :  0\n",
      "clickbait  length :  2401.6675543557567\n",
      "hate  length :  4541.02335117895\n",
      "junksci  length :  3218.29245014245\n",
      "type  length :  7.0\n",
      "2018-02-10 13:43:39.521661  length :  80.0\n",
      "\n",
      "After grouping labels have been applied:\n",
      "reliable  length :  3200.7542642865096\n",
      "fake  length :  2725.9594275287373\n"
     ]
    }
   ],
   "source": [
    "df_995_cleaned_preprocessed_explore = pd.read_csv('995_cleaned_preprocessed.csv')\n",
    "\n",
    "def avg_type_length(df, type):\n",
    "    n_type = 0\n",
    "    type_length = 0\n",
    "    for i in range(len(df)):\n",
    "        if df[\"type\"].iloc[i] == type:\n",
    "            type_length += len(df[\"content\"].iloc[i])\n",
    "            n_type += 1\n",
    "    avg_length = type_length / n_type if n_type != 0 else 0\n",
    "    return avg_length\n",
    "\n",
    "type_list = df_995_cleaned_preprocessed_explore[\"type\"].unique()\n",
    "\n",
    "print(\"Before grouping labels:\")\n",
    "for word in type_list:\n",
    "    print(word, \" length : \", avg_type_length(df_995_cleaned_preprocessed_explore, word))\n",
    "\n",
    "df_995_cleaned_preprocessed_explore = group_labels_995(df_995_cleaned_preprocessed_explore)\n",
    "\n",
    "print(\"\\nAfter grouping labels have been applied:\")\n",
    "\n",
    "type_list = df_995_cleaned_preprocessed_explore[\"type\"].unique()\n",
    "for word in type_list:\n",
    "    print(word, \" length : \", avg_type_length(df_995_cleaned_preprocessed_explore, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed = pd.read_csv('995_cleaned_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['political', 'fake', 'satire', 'reliable', 'conspiracy',\n",
       "       'unreliable', 'bias', 'rumor', 'unknown', nan, 'clickbait', 'hate',\n",
       "       'junksci', 'type', '2018-02-10 13:43:39.521661'], dtype=object)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_995_cleaned_preprocessed['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1, Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed = group_labels_995(df_995_cleaned_preprocessed)\n",
    "\n",
    "min_count = min(df_995_cleaned_preprocessed['type'].value_counts())\n",
    "\n",
    "df_995_cleaned_preprocessed_balanced_simple = pd.concat([\n",
    "    df_995_cleaned_preprocessed[df_995_cleaned_preprocessed['type'] == 'fake'].sample(n=min_count, random_state=1),\n",
    "    df_995_cleaned_preprocessed[df_995_cleaned_preprocessed['type'] == 'reliable'].sample(n=min_count, random_state=1)\n",
    "])\n",
    "\n",
    "df_995_cleaned_preprocessed_balanced_simple = pre_process(df_995_cleaned_preprocessed_balanced_simple, \"content\")\n",
    "df_995_cleaned_preprocessed_balanced_simple[\"wordcount\"] = df_995_cleaned_preprocessed_balanced_simple[\"content\"].apply(len)\n",
    "\n",
    "X_simple = df_995_cleaned_preprocessed_balanced_simple[[\"wordcount\"]]\n",
    "\n",
    "labels_simple = []\n",
    "for i in df_995_cleaned_preprocessed_balanced_simple['type']:\n",
    "    if i == 'fake':\n",
    "        labels_simple.append(1)\n",
    "    else:\n",
    "        labels_simple.append(0)\n",
    "y_simple = labels_simple\n",
    "\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(X_simple, y_simple, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2, Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uniques(df, column):\n",
    "    unique_words = Counter()\n",
    "    for words in df[column]:\n",
    "        for word in words:\n",
    "            if isinstance(word, str) and word.isalpha() and len(word) > 4:\n",
    "                unique_words.update([word])\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_guesser(df, column, type, liar=False):\n",
    "    \n",
    "    if liar == True:\n",
    "        df_fake = df[df[type] == 'fake']\n",
    "        df_reliable = df[df[type] == 'reliable']\n",
    "        xyz = count_uniques_after_clean(df_fake, column)\n",
    "        zyx = count_uniques_after_clean(df_reliable, column)\n",
    "        top_50_fake = xyz.most_common(200)\n",
    "        top_50_reliable= zyx.most_common(200)\n",
    "    \n",
    "        fake_50 = [str_[0] for str_ in top_50_fake]\n",
    "        reliable_50 = [str_[0] for str_ in top_50_reliable]\n",
    "        df[\"fakeCount\"] = df[column].apply(lambda x: sum(word in fake_50 for word in x))\n",
    "        df[\"reliableCount\"] = df[column].apply(lambda x: sum(word in reliable_50 for word in x))\n",
    "        df[\"simple guess\"] = np.where(df[\"fakeCount\"] > df[\"reliableCount\"], \"fake\", \"reliable\")\n",
    "        count_fake = (df[type] == \"fake\").sum()\n",
    "        counter = ((df[type] == \"fake\") & (df[\"simple guess\"] == \"fake\")).sum()\n",
    "        accuracy = counter / count_fake\n",
    "        return accuracy\n",
    "        \n",
    "    else:\n",
    "        df_fake = df[df['type'] == 'fake']\n",
    "        df_reliable = df[df['type'] == 'reliable']\n",
    "        xyz = count_unique_words_in_list_guesser(df_fake, column)\n",
    "        zyx = count_unique_words_in_list_guesser(df_reliable, column)\n",
    "        top_50_fake = xyz.most_common(200)\n",
    "        top_50_reliable= zyx.most_common(200)\n",
    "\n",
    "        fake_50 = [str_[0] for str_ in top_50_fake]\n",
    "        reliable_50 = [str_[0] for str_ in top_50_reliable]\n",
    "        df[\"fakeCount\"] = df[column].apply(lambda x: sum(word in fake_50 for word in x.split()))\n",
    "        df[\"reliableCount\"] = df[column].apply(lambda x: sum(word in reliable_50 for word in x.split()))\n",
    "        df[\"simple guess\"] = np.where(df[\"fakeCount\"] > df[\"reliableCount\"], \"fake\", \"reliable\")\n",
    "        count_fake = (df[\"type\"] == \"fake\").sum()\n",
    "        counter = ((df[\"type\"] == \"fake\") & (df[\"simple guess\"] == \"fake\")).sum()\n",
    "        accuracy = counter / count_fake\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5326045827202018\n"
     ]
    }
   ],
   "source": [
    "model_simple = LogisticRegression()\n",
    "model_simple.fit(X_train_simple, y_train_simple)\n",
    "y_pred_simple = model_simple.predict(X_test_simple)\n",
    "accuracy_simple = accuracy_score(y_test_simple, y_pred_simple)\n",
    "print(accuracy_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed_simple = pd.read_csv('995_cleaned_preprocessed.csv', nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed_simple = group_labels_995(df_995_cleaned_preprocessed_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611749680715197\n"
     ]
    }
   ],
   "source": [
    "print(simple_guesser(df_995_cleaned_preprocessed_simple, \"content\", \"type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2, Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed_bbc = pd.read_csv('995_cleaned_preprocessed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_articles = pd.read_csv('bbc_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbc = pd.DataFrame({'content': bbc_articles.Text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed_bbc = pd.concat([df_995_cleaned_preprocessed_bbc, df_bbc], ignore_index=True)\n",
    "df_995_cleaned_preprocessed_bbc.loc[len(df_995_cleaned_preprocessed_bbc)-len(df_bbc):, 'type'] = 'reliable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995_cleaned_preprocessed_bbc = group_labels_995(df_995_cleaned_preprocessed_bbc)\n",
    "\n",
    "min_count_bbc = min(df_995_cleaned_preprocessed_bbc['type'].value_counts())\n",
    "\n",
    "df_995_cleaned_preprocessed_balanced_simple_bbc = pd.concat([\n",
    "    df_995_cleaned_preprocessed_bbc[df_995_cleaned_preprocessed_bbc['type'] == 'fake'].sample(n=min_count_bbc, random_state=1),\n",
    "    df_995_cleaned_preprocessed_bbc[df_995_cleaned_preprocessed_bbc['type'] == 'reliable'].sample(n=min_count_bbc, random_state=1)\n",
    "])\n",
    "\n",
    "df_995_cleaned_preprocessed_balanced_simple_bbc = pre_process(df_995_cleaned_preprocessed_balanced_simple_bbc, \"content\")\n",
    "df_995_cleaned_preprocessed_balanced_simple_bbc[\"wordcount\"] = df_995_cleaned_preprocessed_balanced_simple_bbc[\"content\"].apply(len)\n",
    "\n",
    "X_simple_bbc = df_995_cleaned_preprocessed_balanced_simple_bbc[[\"wordcount\"]]\n",
    "\n",
    "labels_simple_bbc = []\n",
    "for i in df_995_cleaned_preprocessed_balanced_simple_bbc['type']:\n",
    "    if i == 'fake':\n",
    "        labels_simple_bbc.append(1)\n",
    "    else:\n",
    "        labels_simple_bbc.append(0)\n",
    "y_simple_bbc = labels_simple_bbc\n",
    "\n",
    "X_train_simple_bbc, X_test_simple_bbc, y_train_simple_bbc, y_test_simple_bbc = train_test_split(X_simple_bbc, y_simple_bbc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5321000630649569\n"
     ]
    }
   ],
   "source": [
    "model_simple_bbc = LogisticRegression()\n",
    "model_simple_bbc.fit(X_train_simple_bbc, y_train_simple_bbc)\n",
    "y_pred_simple_bbc = model_simple_bbc.predict(X_test_simple_bbc)\n",
    "accuracy_simple_bbc = accuracy_score(y_test_simple_bbc, y_pred_simple_bbc)\n",
    "print(accuracy_simple_bbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake = df_995_cleaned_preprocessed[df_995_cleaned_preprocessed['type'] == \"fake\"].shape[0]\n",
    "reliable_sampled = df_995_cleaned_preprocessed[df_995_cleaned_preprocessed['type'] == \"reliable\"].sample(n=n_fake)\n",
    "\n",
    "balanced_df_advanced = pd.concat([df_995_cleaned_preprocessed[df_995_cleaned_preprocessed['type'] == \"fake\"], reliable_sampled])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "X_advanced = tfidf_vectorizer.fit_transform(balanced_df_advanced[\"content\"])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "advanced_labels = []\n",
    "for i in balanced_df_advanced['type']:\n",
    "    if i == 'fake':\n",
    "        advanced_labels.append(1)\n",
    "    else:\n",
    "        advanced_labels.append(0)\n",
    "y_advanced = advanced_labels\n",
    "\n",
    "\n",
    "X_train_advanced, X_test_advanced, y_train_advanced, y_test_advanced = train_test_split(X_advanced, y_advanced, test_size=0.2, random_state=0)\n",
    "X_val_advanced, X_test_advanced, y_val_advanced, y_test_advanced = train_test_split(X_test_advanced, y_test_advanced, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters logReg: penalty: l1, C: 1, solver: liblinear\n",
      "Best accuracy logReg: 0.9129283161656506\n"
     ]
    }
   ],
   "source": [
    "logReg_pipe = Pipeline([\n",
    "    ('logistic', LogisticRegression(max_iter=10000))\n",
    "])\n",
    "logReg_parameters = [\n",
    "    \n",
    "    {'logistic__penalty': ['l2'],\n",
    "    'logistic__C': [0.1, 1, 10],\n",
    "    'logistic__solver': 'lbfgs'},\n",
    "\n",
    "    {'logistic__penalty': ['l1', 'l2'],\n",
    "    'logistic__C': [0.1, 1, 10],\n",
    "    'logistic__solver': 'liblinear'},                 \n",
    "    \n",
    "    {'logistic__penalty': ['l2'],\n",
    "    'logistic__C': [0.1, 1, 10],\n",
    "    'logistic__solver': 'newton-cg'}, \n",
    "\n",
    "    {'logistic__penalty': ['l2'],\n",
    "    'logistic__C': [0.1, 1, 10],\n",
    "    'logistic__solver': 'sag'}\n",
    "    \n",
    "    ]\n",
    "logReg_best_accuracy = 0\n",
    "logReg_best_hyperparameter = ''\n",
    "for param in logReg_parameters:\n",
    "    for penalt in param['logistic__penalty']:\n",
    "        if penalt == 'none':\n",
    "            reg = logReg_pipe.set_params(logistic__penalty=penalt, logistic__solver=param['logistic__solver']).fit(X_train_advanced, y_train_advanced)\n",
    "            val_pred = reg.predict(X_val_advanced)\n",
    "            val_acc = accuracy_score(y_val_advanced, val_pred)\n",
    "            if val_acc > logReg_best_accuracy:\n",
    "                logReg_best_accuracy = val_acc\n",
    "                logReg_best_hyperparameter = f\"penalty: {penalt}, C: default, solver: {param} \"\n",
    "        else:\n",
    "            for C_ in param['logistic__C']:\n",
    "                reg = logReg_pipe.set_params(logistic__solver=param['logistic__solver'], logistic__penalty=penalt, logistic__C=C_).fit(X_train_advanced, y_train_advanced)\n",
    "                val_pred = reg.predict(X_val_advanced)\n",
    "                val_acc = accuracy_score(y_val_advanced, val_pred)\n",
    "                if val_acc > logReg_best_accuracy:\n",
    "                    logReg_best_accuracy = val_acc\n",
    "                    logReg_best_hyperparameter = f\"penalty: {penalt}, C: {C_}, solver: {param['logistic__solver']}\"\n",
    "print(\"Best parameters logReg:\", logReg_best_hyperparameter)\n",
    "print(\"Best accuracy logReg:\", logReg_best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters NB: alpha: 0.1, binarize: None\n",
      "Best accuracy NB: 0.8462896783687198\n"
     ]
    }
   ],
   "source": [
    "nb_pipe = Pipeline([\n",
    "    ('bernoulli_nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "nb_parameters = [\n",
    "    {'bernoulli_nb__alpha': [0.1, 1.0, 10.0], 'bernoulli_nb__binarize': [0.0, 0.5, 1.0]},\n",
    "    {'bernoulli_nb__alpha': [0.1, 1.0, 10.0], 'bernoulli_nb__binarize': [None]}\n",
    "]\n",
    "\n",
    "nb_best_accuracy = 0\n",
    "nb_best_hyperparameter = ''\n",
    "\n",
    "for param in nb_parameters:\n",
    "    for alpha_ in param['bernoulli_nb__alpha']:\n",
    "        for binarize_ in param['bernoulli_nb__binarize']:\n",
    "            reg = nb_pipe.set_params(bernoulli_nb__alpha=alpha_, bernoulli_nb__binarize=binarize_).fit(X_train_advanced, y_train_advanced)\n",
    "            val_pred = reg.predict(X_val_advanced)\n",
    "            val_acc = accuracy_score(y_val_advanced, val_pred)\n",
    "            if val_acc > nb_best_accuracy:\n",
    "                nb_best_accuracy = val_acc\n",
    "                nb_best_hyperparameter = f\"alpha: {alpha_}, binarize: {binarize_}\"\n",
    "\n",
    "print(\"Best parameters NB:\", nb_best_hyperparameter)\n",
    "print(\"Best accuracy NB:\", nb_best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4, Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9126760563380282\n"
     ]
    }
   ],
   "source": [
    "best_LogisticRegression_model = LogisticRegression(penalty= 'l1', C = 1, solver = 'liblinear').fit(X_train_advanced, y_train_advanced)\n",
    "best_LogisticRegression_predict = best_LogisticRegression_model.predict(X_test_advanced)\n",
    "best_LogisticRegression_accuracy = accuracy_score(y_test_advanced, best_LogisticRegression_predict)\n",
    "print(best_LogisticRegression_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8444397729661551\n"
     ]
    }
   ],
   "source": [
    "best_BernoulliNB_model = BernoulliNB(alpha = 0.1, binarize=None).fit(X_train_advanced, y_train_advanced)\n",
    "best_BernoulliNB_predict = best_BernoulliNB_model.predict(X_test_advanced)\n",
    "best_BernoulliNB_accuracy = accuracy_score(y_test_advanced, best_BernoulliNB_predict)\n",
    "print(best_BernoulliNB_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4, Task2 and Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_labels_liar(dataframe):\n",
    "    replace_fake = re.compile(r'(false)|(barely-true)|(pants-fire)')\n",
    "    replace_reliable = re.compile(r'(true)|(mostly-true)')\n",
    "    replace_omitted = re.compile(r'(half-true)|(half-reliable)')\n",
    "    dataframe[1] = dataframe[1].apply(lambda x: re.sub(replace_fake, 'fake', str(x)))\n",
    "    dataframe[1] = dataframe[1].apply(lambda x: re.sub(replace_reliable, 'reliable', str(x)))\n",
    "    dataframe[1] = dataframe[1].apply(lambda x: re.sub(replace_omitted, 'omitted', str(x)))\n",
    "    remove_omitted = dataframe[(dataframe[1] == 'omitted')].index\n",
    "    dataframe.drop(remove_omitted, inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_labels(dataframe, column):\n",
    "    labels = []\n",
    "    for i in dataframe[column]:\n",
    "        if i == 'fake':\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liar_simplev1 = pd.read_csv(\"test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liar_simplev1 = clean_text(df_liar_simplev1, 2)\n",
    "df_liar_simplev1 = group_labels_liar(df_liar_simplev1)\n",
    "df_liar_simplev1 = pre_process(df_liar_simplev1, 2, tokeniz=True, stopw=True, stemming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model v1: 0.49547920433996384\n"
     ]
    }
   ],
   "source": [
    "print(\"simple model v1:\", simple_guesser(df_liar_simplev1, 2, 1, liar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model v2: 0.5518962075848304\n"
     ]
    }
   ],
   "source": [
    "df_liar_simplev2 = pd.read_csv(\"test.tsv\", sep=\"\\t\", header=None)\n",
    "df_liar_simplev2 = clean_text(df_liar_simplev2, 2)\n",
    "df_liar_simplev2 = group_labels_liar(df_liar_simplev2)\n",
    "df_liar_simplev2 = pre_process(df_liar_simplev2, 2, tokeniz=True, stopw=True, stemming=True)\n",
    "liar_labels_simplev2 = calc_labels(df_liar_simplev2, 1)\n",
    "\n",
    "df_liar_simplev2[\"wordcount\"] = df_liar_simplev2[2].apply(len)\n",
    "\n",
    "liar_X_test_simplev2 = df_liar_simplev2[[\"wordcount\"]]\n",
    "\n",
    "simple_model_liarv2 = model_simple.predict(liar_X_test_simplev2)\n",
    "accuracy_simple_liarv2 = accuracy_score(liar_labels_simplev2, simple_model_liarv2)\n",
    "print(\"simple model v2:\", accuracy_simple_liarv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liar_advanced = pd.read_csv(\"test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liar_advanced = clean_text(df_liar_advanced, 2)\n",
    "df_liar_advanced = group_labels_liar(df_liar_advanced)\n",
    "liar_labels_advanced = calc_labels(df_liar_advanced, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_X_test_advanced = tfidf_vectorizer.transform(df_liar_advanced[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4540918163672655\n"
     ]
    }
   ],
   "source": [
    "logistic_liar_y_pred = best_LogisticRegression_model.predict(liar_X_test_advanced)\n",
    "logistic_liar_accuracy = accuracy_score(liar_labels_advanced, logistic_liar_y_pred)\n",
    "print(logistic_liar_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5499001996007984\n"
     ]
    }
   ],
   "source": [
    "NB_liar_y_pred = best_BernoulliNB_model.predict(liar_X_test_advanced)\n",
    "NB_liar_accuracy = accuracy_score(liar_labels_advanced, NB_liar_y_pred)\n",
    "print(NB_liar_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
